# nanoGPT
This is my implementation of nanoGPT by following Andrej Karpathy's excellent YouTube video [Let's build GPT: from scratch, in code, spelled out.](https://www.youtube.com/watch?v=kCc8FmEb1nY&t=1s)

## Dataset
The training dataset, named "input.txt," comprises the complete works of Shakespeare in a single text file.

## Model Architecture
The transformer model is constructed from the ground up, focusing solely on the decoder component. The architecture closely adheres to the foundational paper "[Attention is All You Need](https://arxiv.org/abs/1706.03762)," with several minor optimizations incorporated.

## To Do
- [ ] Implement the encoding component
- [ ] Train the model using an alternative dataset

# nanoGPT
This is my implementation of nanoGPT by following Andrej Karpathy's excellent YouTube video [Let's build GPT: from scratch, in code, spelled out.](https://www.youtube.com/watch?v=kCc8FmEb1nY&t=1s)

- "input.txt" is the dataset used. It is the entire work of Shakespeare in a text file.
- The transformer model is built from scratch with just the decoder part and it closely follows the original paper [Attention is all you need](https://arxiv.org/abs/1706.03762) with minor optimizations.

- To Do
  - Add the encode part
  - train it on a different dataset
